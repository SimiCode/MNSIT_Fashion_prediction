{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training Convolutional Neural Networks to Categorize Clothing with PyTorch</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Creating the Convolutional Neural Network:</b>\n",
    "I’ll be showing you how I built my convolutional neural network in Pytorch. I trained it using the MNIST — Fashion dataset with 60,000 examples of 28x28 resolution black-and-white images of clothes. Let’s jump right in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin with the imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we initialize the hyperparameters, which include the number of epochs (training rounds), number of classes (there are 10: t-shirt, trouser, pullover, dress, coat, sandal, shirt, sneaker, bag, and boot), the batch-size for mini-batch gradient descent, and the learning rate for gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing hyperparameters\n",
    "# number of times to pass\n",
    "num_epochs = 8\n",
    "# number of categories to group into\n",
    "num_classes = 10\n",
    "\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to retrieve the dataset! We load both the training set and the test set of the MNIST-Fashion and set the transform parameter to convert the datasets into tensors (transforms.ToTensor()). We also normalize them by setting the mean and standard deviation (transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Download and load dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root='./data', \n",
    "                            train=True, \n",
    "                            download=True,\n",
    "                            transform=transform)\n",
    "\n",
    "test_dataset = datasets.FashionMNIST(root='./data', \n",
    "                           train=False, \n",
    "                           download=True,\n",
    "                           transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step in this stage is to place the data into an object (data loader) to make it more easily accessible. We shuffle the training dataset so that there won’t be any bias in the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading dataset into dataloader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Part 2: Constructing the Convolutional Neural Network</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize our neural network using the nn.Module class, the base class of all layered neural network modules:\n",
    "\n",
    "Our convolutional network will two convolution layers, each followed by a non-linear function (ReLu) and a max-pooling layer, and finally a fully connected layer and softmax for linear regression.\n",
    "\n",
    "\n",
    "Layers of the Convolutional Neural Network\n",
    "We also use dropout for regularization before the fully connected layer to prevent against overfitting. Thus, we can initialize the layers in the network as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your cnn model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        #Convolution 1\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        #Max pool 1\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        #Convolution 2\n",
    "        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        #Max pool 2\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        #Dropout for regularization\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        #Fully Connected 1\n",
    "        self.fc1 = nn.Linear(32*7*7, 10)\n",
    "        \n",
    "    '''In our forward function for forward propagation, we apply each layer to the input data, \n",
    "    as well as the dropout. Additionally, before the regularization, \n",
    "    we flatten the data to be one-dimensional for the linear regression (the first dimension is the batch size).'''\n",
    "    def forward(self, x):\n",
    "        #Convolution 1\n",
    "        out = self.cnn1(x)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        #Max pool 1\n",
    "        out = self.maxpool1(out)\n",
    "        \n",
    "        #Convolution 2\n",
    "        out = self.cnn2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        #Max pool 2\n",
    "        out = self.maxpool2(out)\n",
    "        \n",
    "        #Resize\n",
    "        out = out.view(out.size(0), -1)\n",
    "        \n",
    "        #Dropout\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        #Fully connected 1\n",
    "        out = self.fc1(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Part 3: Creating Instances</b>\n",
    "\n",
    "Now that we’ve created a class for our convolutional neural network, we need to create an instance of it (we’ve created a class to determine its layers and forward propagation, but we haven’t actually created an actual neural net yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create instance of model\n",
    "model = CNNModel()\n",
    "\n",
    "'''We also use cross-entropy loss to determine the labels from the output of the neural net.'''\n",
    "#Create instance of loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''Finally, we initialize the linear regression/softmax function.'''\n",
    "#Create instance of optimizer (Adam)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Part 4: Training the Model</b>\n",
    "\n",
    "After creating the network and the instances, we’re ready to train it using the dataset! We iterate through the dataset and for each mini-batch, we perform forward propagation, calculate the cross-entropy loss, perform backward propagation and use the gradients to update the parameters."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Also, for every 100 mini-batches, the loss and accuracy of the neural network. The final outputs should look something like this:\n",
    "Epoch [1/8], Step [400/600], Loss: 0.3835, Accuracy: 88.00%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8], Step [100/600], Loss: 0.6973, Accuracy: 76.00%\n",
      "Epoch [1/8], Step [200/600], Loss: 0.4977, Accuracy: 79.00%\n",
      "Epoch [1/8], Step [300/600], Loss: 0.5466, Accuracy: 81.00%\n",
      "Epoch [1/8], Step [400/600], Loss: 0.3835, Accuracy: 88.00%\n",
      "Epoch [1/8], Step [500/600], Loss: 0.4047, Accuracy: 86.00%\n",
      "Epoch [1/8], Step [600/600], Loss: 0.3863, Accuracy: 88.00%\n",
      "Epoch [2/8], Step [100/600], Loss: 0.3510, Accuracy: 86.00%\n",
      "Epoch [2/8], Step [200/600], Loss: 0.3373, Accuracy: 87.00%\n",
      "Epoch [2/8], Step [300/600], Loss: 0.3519, Accuracy: 85.00%\n",
      "Epoch [2/8], Step [400/600], Loss: 0.2520, Accuracy: 90.00%\n",
      "Epoch [2/8], Step [500/600], Loss: 0.3548, Accuracy: 87.00%\n",
      "Epoch [2/8], Step [600/600], Loss: 0.2723, Accuracy: 90.00%\n",
      "Epoch [3/8], Step [100/600], Loss: 0.4901, Accuracy: 82.00%\n",
      "Epoch [3/8], Step [200/600], Loss: 0.3767, Accuracy: 82.00%\n",
      "Epoch [3/8], Step [300/600], Loss: 0.5188, Accuracy: 85.00%\n",
      "Epoch [3/8], Step [400/600], Loss: 0.4118, Accuracy: 87.00%\n",
      "Epoch [3/8], Step [500/600], Loss: 0.3100, Accuracy: 87.00%\n",
      "Epoch [3/8], Step [600/600], Loss: 0.2979, Accuracy: 88.00%\n",
      "Epoch [4/8], Step [100/600], Loss: 0.2850, Accuracy: 91.00%\n",
      "Epoch [4/8], Step [200/600], Loss: 0.2919, Accuracy: 89.00%\n",
      "Epoch [4/8], Step [300/600], Loss: 0.4656, Accuracy: 84.00%\n",
      "Epoch [4/8], Step [400/600], Loss: 0.2051, Accuracy: 92.00%\n",
      "Epoch [4/8], Step [500/600], Loss: 0.2245, Accuracy: 91.00%\n",
      "Epoch [4/8], Step [600/600], Loss: 0.4634, Accuracy: 83.00%\n",
      "Epoch [5/8], Step [100/600], Loss: 0.3697, Accuracy: 87.00%\n",
      "Epoch [5/8], Step [200/600], Loss: 0.4071, Accuracy: 85.00%\n",
      "Epoch [5/8], Step [300/600], Loss: 0.2831, Accuracy: 88.00%\n",
      "Epoch [5/8], Step [400/600], Loss: 0.2635, Accuracy: 90.00%\n",
      "Epoch [5/8], Step [500/600], Loss: 0.3872, Accuracy: 88.00%\n",
      "Epoch [5/8], Step [600/600], Loss: 0.1859, Accuracy: 94.00%\n",
      "Epoch [6/8], Step [100/600], Loss: 0.2367, Accuracy: 92.00%\n",
      "Epoch [6/8], Step [200/600], Loss: 0.1978, Accuracy: 94.00%\n",
      "Epoch [6/8], Step [300/600], Loss: 0.2873, Accuracy: 91.00%\n",
      "Epoch [6/8], Step [400/600], Loss: 0.0899, Accuracy: 99.00%\n",
      "Epoch [6/8], Step [500/600], Loss: 0.2499, Accuracy: 89.00%\n",
      "Epoch [6/8], Step [600/600], Loss: 0.2940, Accuracy: 90.00%\n",
      "Epoch [7/8], Step [100/600], Loss: 0.3123, Accuracy: 88.00%\n",
      "Epoch [7/8], Step [200/600], Loss: 0.3857, Accuracy: 85.00%\n",
      "Epoch [7/8], Step [300/600], Loss: 0.2634, Accuracy: 92.00%\n",
      "Epoch [7/8], Step [400/600], Loss: 0.1277, Accuracy: 97.00%\n",
      "Epoch [7/8], Step [500/600], Loss: 0.3392, Accuracy: 90.00%\n",
      "Epoch [7/8], Step [600/600], Loss: 0.2535, Accuracy: 90.00%\n",
      "Epoch [8/8], Step [100/600], Loss: 0.2224, Accuracy: 92.00%\n",
      "Epoch [8/8], Step [200/600], Loss: 0.2249, Accuracy: 92.00%\n",
      "Epoch [8/8], Step [300/600], Loss: 0.1786, Accuracy: 93.00%\n",
      "Epoch [8/8], Step [400/600], Loss: 0.3008, Accuracy: 89.00%\n",
      "Epoch [8/8], Step [500/600], Loss: 0.2851, Accuracy: 87.00%\n",
      "Epoch [8/8], Step [600/600], Loss: 0.3395, Accuracy: 90.00%\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        #Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Forward propagation \n",
    "        outputs = model(images)      \n",
    "        \n",
    "        #Calculating loss with softmax to obtain cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        #Backward propation\n",
    "        loss.backward()\n",
    "        \n",
    "        #Updating gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        #Total number of labels\n",
    "        total = labels.size(0)\n",
    "        \n",
    "        #Obtaining predictions from max value\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        #Calculate the number of correct answers\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        \n",
    "        #Print loss and accuracy\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, len(train_loader), loss.item(),\n",
    "                          (correct / total) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Part 5: Testing the Model</b>\n",
    "\n",
    "So our model is trained, all that’s left to do is to test it! We run the neural network on the test dataset, compare our outputs to the correct labels, and determine our overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 89.46 %\n"
     ]
    }
   ],
   "source": [
    "#Testing the model\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network should achieve an accuracy from 88%–90%, which is quite good compared to the 91.6% benchmark of 2-layered CNNs on the MNIST-Fashion dataset. Note that the MNIST-Fashion dataset is much harder to train on than the original MNIST-digit dataset. If we want to achieve a higher accuracy, we would have to add more layers, preprocess the data more to normalize it better, and increase the number of epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
